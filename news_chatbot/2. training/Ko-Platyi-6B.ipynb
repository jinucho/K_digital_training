{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=04jCXo5kzZE&list=PLQIgLu3Wf-q_Ne8vv-ZXuJ4mztHJaQb_v&index=12"],"metadata":{"id":"1sGUTGI9LsSJ"}},{"cell_type":"markdown","source":["# 자료 인스톨"],"metadata":{"id":"FDD2Y7Ww9QxU"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"sMKHOMTS9XgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","torch.__version__"],"metadata":{"id":"0FBPoFqR9ZXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YnhMtsQvKg-"},"outputs":[],"source":["!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git"]},{"cell_type":"markdown","source":["# 2.양자화"],"metadata":{"id":"9EPd-T9l9Tll"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n","                                bnb_4bit_use_double_quant=True,\n","                                bnb_4bit_quant_type=\"nf4\",\n","                                bnb_4bit_compute_dtype=torch.bfloat16\n","                                )"],"metadata":{"id":"cEJms2m2v6kl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain\n","!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes"],"metadata":{"id":"s1909H_96ImJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.LLM 모델 구성"],"metadata":{"id":"AmOCaiqP9WvM"}},{"cell_type":"code","source":["model_id = \"kyujinpy/Ko-PlatYi-6B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n"],"metadata":{"id":"zq1bMpqryOHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"id":"pAgYKFhzfdx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AIxNYjn5fhBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RqzeBsi-fhER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6nRLFBytfhHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oOjrDAmVfhKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DmfKR-8EfhNR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","messages = [ {\"role\": \"user\", \"content\" : \"은행의 기준 금리에 대해서 설명해줘\"}]\n","\n","encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\" )\n","model_inputs = encodeds.to(device)\n","\n","generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample= True)\n","decoded = tokenizer.batch_decode(generated_ids)\n","\n","decoded[0]"],"metadata":{"id":"aLvZfJmhyOKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"glTtypVyfgMR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.LangChian 구성"],"metadata":{"id":"SVUX_iV09as9"}},{"cell_type":"code","source":["import locale\n","\n","def getpreferredencoding(do_setlocale =True):\n","  return \"UTF-8\"\n","\n","locale.getpreferredencoding = getpreferredencoding"],"metadata":{"id":"YsYKyBvGyOPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip -q install langchain pypdf chromadb sentence-transformers faiss-gpu\n","# !pip install transformers==4.28.0"],"metadata":{"id":"cd3u5U96yOSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import HuggingFacePipeline\n","from langchain.prompts import PromptTemplate\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from langchain.chains import LLMChain\n","from transformers import pipeline\n","\n","text_generation_pipeline = pipeline(\n","    model= model,\n","    tokenizer = tokenizer,\n","    task = \"text-generation\", #\"summarization\",\n","    temperature =0.2,\n","    return_full_text=True,\n","    max_new_tokens = 300,\n","    )\n","\n","# Yi prompt (https://replicate.com/01-ai/yi-34b-chat)\n","#  <|im_start|>system\n","#  You are a helpful assistant<|im_end|>\n","#  <|im_start|>user\n","#  {prompt}<|im_end|>\n","#  <|im_start|>assistant\n","\n","prompt_template =\"\"\"\n","### [INST]/\n","Instruction : Answer the question based on your knowledge.\n","Here is context to help:\n","{context}\n","\n","### QUESTION:\n","{question}\n","\n","[/INST]\n","\"\"\"\n","\n","koplatyi_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","\n","# Create prompt from prompt template\n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template = prompt_template,\n",")\n","\n","# Create llm chain\n","llm_chain = LLMChain(llm=koplatyi_llm, prompt=prompt)\n"],"metadata":{"id":"-h_dUPfuyOVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install unstructured"],"metadata":{"id":"5Ub4VitxB-se"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.DB Loading"],"metadata":{"id":"K1eRFkF79emk"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","from langchain.document_loaders import PyPDFLoader # PDF document loader\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain_community.document_loaders import UnstructuredExcelLoader # Excel document loader"],"metadata":{"id":"wKCZ2fmuyOYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loader = PyPDFLoader(\"/content/outlooks.pdf\")\n","# # loader = PyPDFLoader(\"/content/economic-outlook2024.pdf\")\n","# pages = loader.load_and_split()\n","\n","# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","# texts = text_splitter.split_documents(pages)\n","\n","loader = UnstructuredExcelLoader(\"/content/news_2024-03-08_99_99.xlsx\") #, mode=\"elements\")\n","loader\n"],"metadata":{"id":"6an1nyy8yObM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n","texts = text_splitter.split_documents(docs)\n","for tx in texts[:5]:\n","  display(tx)\n","  print()"],"metadata":{"id":"QsP5QJ2qAnzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.임베딩 구성 및 Data"],"metadata":{"id":"vAt9wHzJp4VJ"}},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","\n","model_name = \"jhgan/ko-sbert-nli\"\n","# model_name = \"digit82/kobart-summarization\"\n","\n","encode_kwargs ={\"normalize_embeddings\" : True}\n","hf = HuggingFaceEmbeddings(\n","    model_name = model_name,\n","    encode_kwargs=encode_kwargs,\n"," )\n","\n","# from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, AutoTokenizer, BertTokenizer\n","# tokenizer = AutoTokenizer.from_pretrained('hyunwoongko/kobart')"],"metadata":{"id":"QlFam0nxp13a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","db = FAISS.from_documents(texts,hf)\n","\n","# CPU times: user 1min 1s, sys: 3.94 s, total: 1min 4s\n","# Wall time: 1min 6s\n","\n","# GPU times: user 7.33 s, sys: 7.51 ms, total: 7.33 s\n","# Wall time: 7.34 s"],"metadata":{"id":"msVPFyLBtHBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","retriever = db.as_retriever(serch_type=\"similarity\",\n","                            serch_kwargs = {\"k\":3}\n","                            )\n","\n","# prompt = ChatPromptTemplate.from_template(template)\n","rag_chain = (\n","             {\"context\":retriever, \"question\": RunnablePassthrough()}\n","             | llm_chain\n","             )\n","# 맥락: retriever를 통해 결정\n","# 질문: RunnablePassthrough 접수\n","# llm_chain으로 처리를 하라\n","\n"],"metadata":{"id":"zGZaZs6JBHdt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","result = rag_chain.invoke(\"가상화폐관련 뉴스는 무엇인가?\")\n","print(f\"**답변**\\n>>>>>{result['text']}\\n\\n\")\n","\n","for i in result['context']:\n","  # print(i)\n","  print(f\" ■ 근거: {i.page_content}\\n ■ 출처: {i.metadata['source']} \\n\") # - {i.metadata['page']} \\n\\n\")"],"metadata":{"id":"iu9LBIJ2F54C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i"],"metadata":{"id":"Sxrc-KZSBHwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NB303Pj5BHzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nE6JReskBH2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mu7wXodRyOhN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tm2mg_olyOkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yfidSJFZHFts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jAUPfORuHFxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id = \"kyujinpy/Ko-PlatYi-6B\"\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=device)\n","# print(model)"],"metadata":{"id":"pZt8kpcMHF0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kStTWR9gHF3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.memory import ConversationSummaryBufferMemory\n","from langchain.chat_models import ChatOpenAI\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"],"metadata":{"id":"amvMop8oHF6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generated_ids\n","# decoded\n","\n","memory = ConversationSummaryBufferMemory(\n","    llm=model,\n","    max_token_limit=400,\n","    memory_key=\"chat_history\",\n","    return_messages=True,\n",")\n","\n","def load_memory(input):\n","    print(input)\n","    return memory.load_memory_variables({})[\"chat_history\"]\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful AI talking to human\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{question}\"),\n","])\n","\n","chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n","\n","def invoke_chain(question):\n","    result = chain.invoke({\"question\": question})\n","    memory.save_context(\n","        {\"input\": question},\n","        {\"output\": result.content},\n","    )\n","    print(result)\n"],"metadata":{"id":"Z12rY4xFHF96"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","invoke_chain(\"My name is nam.\")\n","invoke_chain(\"What's my name?\")"],"metadata":{"id":"TuT4boNcQTAa"},"execution_count":null,"outputs":[]}]}